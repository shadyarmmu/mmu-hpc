{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMzczpq7iM/DbyG/DtQM5BQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MS-Om5YFKqy",
        "outputId": "5b693179-cc3d-4005-b1ba-2bc17584cf23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning-utilities\n",
            "  Downloading lightning_utilities-0.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.11/dist-packages (from lightning-utilities) (25.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities) (4.14.1)\n",
            "Downloading lightning_utilities-0.15.0-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities\n",
            "Successfully installed lightning-utilities-0.15.0\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchmetrics-1.8.0-py3-none-any.whl (981 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.9/981.9 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install lightning-utilities\n",
        "!pip install torchmetrics --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas pillow matplotlib seaborn imagehash tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAQzbdp6jHBw",
        "outputId": "f1902dfd-2b0e-4300-b607-728c3d8a8916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.3.2-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (from imagehash) (1.8.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from imagehash) (1.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading ImageHash-4.3.2-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/296.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imagehash\n",
            "Successfully installed imagehash-4.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HX884t-3lh6",
        "outputId": "0aacc513-de0a-4a62-c1b5-14b4e42fb32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q -P unlocked308BR /content/gdrive/MyDrive/DeepLearning/DFUC2021_trainset_210427.zip -d dfu_train"
      ],
      "metadata": {
        "id": "rTASUX-aHhkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q -P sigmoid608KL /content/gdrive/MyDrive/DeepLearning/DFUC2021_testing_release.zip -d dfu_test"
      ],
      "metadata": {
        "id": "aTxn1p4THjaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Full Self-Supervised Pipeline for DFUC2021 ---\n",
        "\n",
        "# --- SETUP: IMPORTS & CONFIGURATION ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import ViTModel, ViTImageProcessor\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "# Part A: Self-Supervised Config\n",
        "MIM_EPOCHS = 25  # Number of epochs for Masked Image Modeling pre-training. Can be increased for better results.\n",
        "MIM_BATCH_SIZE = 32\n",
        "MODEL_CHECKPOINT = \"google/vit-base-patch16-224-in21k\"\n",
        "ADAPTED_BACKBONE_PATH = \"vit_mae_adapted_backbone.pth\"\n",
        "\n",
        "# Part B & C: Supervised Config\n",
        "PROBE_EPOCHS = 20 # Number of epochs for linear probing on each fold.\n",
        "PROBE_BATCH_SIZE = 32\n",
        "N_SPLITS = 4 # Using 4 folds for cross-validation\n",
        "CLASS_NAMES = ['none', 'infection', 'ischaemia', 'both']\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "# General Config\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "DATA_DIR = \"/content/dfu_train/DFUC2021_train/\"\n",
        "TRAIN_IMG_DIR = os.path.join(DATA_DIR, \"images\")\n",
        "CSV_FILE = os.path.join(DATA_DIR, \"train.csv\")\n",
        "TEST_IMG_DIR = '/content/dfu_test/DFUC2021_test'\n",
        "OUTPUT_CSV_PATH = 'submission_vit_mae_ensemble.csv'\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0pE7LQU-0_z",
        "outputId": "09ed107d-242d-411c-fa21-a33a1f52df3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PART A: SELF-SUPERVISED PRE-TRAINING (MAE-STYLE) ---\n",
        "# This updated version saves the model with the lowest reconstruction loss.\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🚀 PART A: STARTING SELF-SUPERVISED PRE-TRAINING (MAE)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Define MAE Model, Dataset, and Collator\n",
        "class ViTForMAE(nn.Module):\n",
        "    \"\"\"Masked Autoencoder with ViT backbone\"\"\"\n",
        "    def __init__(self, vit_model):\n",
        "        super().__init__()\n",
        "        self.vit = vit_model\n",
        "        self.config = vit_model.config\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_hidden_size = 512 # Can be smaller than encoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.config.hidden_size, self.decoder_hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(self.decoder_hidden_size, self.config.patch_size**2 * 3),\n",
        "        )\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, self.config.hidden_size))\n",
        "\n",
        "    def forward(self, pixel_values, bool_masked_pos):\n",
        "        # Get patch embeddings\n",
        "        embeddings = self.vit.embeddings(pixel_values)\n",
        "\n",
        "        # Exclude CLS token from masking\n",
        "        embeddings_without_cls = embeddings[:, 1:, :]\n",
        "        bool_masked_pos_without_cls = bool_masked_pos[:, 1:]\n",
        "\n",
        "        batch_size, seq_len, dim = embeddings_without_cls.shape\n",
        "        mask = ~bool_masked_pos_without_cls.unsqueeze(-1).expand_as(embeddings_without_cls)\n",
        "        visible_embeddings = embeddings_without_cls[mask].reshape(batch_size, -1, dim)\n",
        "\n",
        "        # Pass visible patches through encoder\n",
        "        encoder_outputs = self.vit.encoder(visible_embeddings)\n",
        "        encoded_visible = encoder_outputs.last_hidden_state\n",
        "\n",
        "        # Add CLS token back to the decoder input\n",
        "        decoder_input = self.mask_token.expand(batch_size, seq_len + 1, -1).clone()\n",
        "        decoder_input[:, 1:][~bool_masked_pos_without_cls] = encoded_visible.flatten(0, 1)\n",
        "\n",
        "        # Pass full sequence through decoder\n",
        "        decoded_patches = self.decoder(decoder_input)\n",
        "\n",
        "        # Predict only the masked patches\n",
        "        predicted_masked_patches = decoded_patches[:, 1:][bool_masked_pos_without_cls]\n",
        "\n",
        "        return predicted_masked_patches\n",
        "\n",
        "class MAEDataCollator:\n",
        "    def __init__(self, processor, vit_config, mask_patch_rate=0.75):\n",
        "        self.processor = processor\n",
        "        self.mask_patch_rate = mask_patch_rate\n",
        "        self.patch_size = vit_config.patch_size\n",
        "        self.num_patches = (vit_config.image_size // self.patch_size) ** 2\n",
        "\n",
        "    def __call__(self, examples):\n",
        "        batch = torch.stack(examples, dim=0)\n",
        "        num_mask = int(self.num_patches * self.mask_patch_rate)\n",
        "\n",
        "        masked_indices = torch.rand(batch.shape[0], self.num_patches).argsort(dim=-1)[:, :num_mask]\n",
        "        bool_masked_pos = torch.zeros((batch.shape[0], self.num_patches + 1), dtype=torch.bool)\n",
        "        for i in range(batch.shape[0]):\n",
        "            bool_masked_pos[i, masked_indices[i] + 1] = True\n",
        "\n",
        "        patches = batch.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        patches = patches.permute(0, 2, 3, 1, 4, 5).reshape(batch.shape[0], self.num_patches, -1)\n",
        "        labels = patches[bool_masked_pos[:, 1:]]\n",
        "\n",
        "        return {\"pixel_values\": batch, \"bool_masked_pos\": bool_masked_pos, \"labels\": labels}\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, processor):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.processor = processor\n",
        "        self.image_files = df['image'].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        return self.processor(image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
        "\n",
        "# 2. Run Self-Supervised Training\n",
        "processor = ViTImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
        "vit_model = ViTModel.from_pretrained(MODEL_CHECKPOINT)\n",
        "mae_model = ViTForMAE(vit_model).to(DEVICE)\n",
        "optimizer_mae = optim.AdamW(mae_model.parameters(), lr=1.5e-4, weight_decay=0.05)\n",
        "loss_fn_mae = nn.MSELoss()\n",
        "\n",
        "full_df = pd.read_csv(CSV_FILE)\n",
        "mae_dataset = ImageDataset(full_df, TRAIN_IMG_DIR, processor)\n",
        "mae_collator = MAEDataCollator(processor, vit_model.config)\n",
        "mae_loader = DataLoader(mae_dataset, batch_size=MIM_BATCH_SIZE, shuffle=True, collate_fn=mae_collator)\n",
        "\n",
        "# --- CHANGE IS HERE: Track best loss ---\n",
        "min_loss = float('inf')\n",
        "\n",
        "for epoch in range(MIM_EPOCHS):\n",
        "    mae_model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(mae_loader, desc=f\"MAE Epoch {epoch+1}/{MIM_EPOCHS}\")\n",
        "    for batch in progress_bar:\n",
        "        pixel_values = batch['pixel_values'].to(DEVICE)\n",
        "        bool_masked_pos = batch['bool_masked_pos'].to(DEVICE)\n",
        "        labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "        optimizer_mae.zero_grad()\n",
        "        predictions = mae_model(pixel_values, bool_masked_pos)\n",
        "        loss = loss_fn_mae(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer_mae.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'Loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "    avg_loss = total_loss / len(mae_loader)\n",
        "    print(f\"MAE Epoch {epoch+1} - Average Reconstruction Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # --- CHANGE IS HERE: Save model only if loss has improved ---\n",
        "    if avg_loss < min_loss:\n",
        "        min_loss = avg_loss\n",
        "        # Save the adapted backbone\n",
        "        torch.save(mae_model.vit.state_dict(), ADAPTED_BACKBONE_PATH)\n",
        "        print(f\"🎉 New best backbone saved to {ADAPTED_BACKBONE_PATH} with loss: {min_loss:.4f}\")\n",
        "\n",
        "print(f\"\\n✅ Self-supervised pre-training complete.\")\n",
        "print(f\"The best adapted backbone is saved at: {ADAPTED_BACKBONE_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        },
        "id": "Q-OiVnp7-45g",
        "outputId": "3685eeec-26e9-4e91-f3d4-4cc4eb03b853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🚀 PART A: STARTING SELF-SUPERVISED PRE-TRAINING (MAE)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MAE Epoch 1/25: 100%|██████████| 311/311 [01:05<00:00,  4.74it/s, Loss=0.0842]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Epoch 1 - Average Reconstruction Loss: 0.1064\n",
            "🎉 New best backbone saved to vit_mae_adapted_backbone.pth with loss: 0.1064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MAE Epoch 2/25: 100%|██████████| 311/311 [01:05<00:00,  4.76it/s, Loss=0.1064]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Epoch 2 - Average Reconstruction Loss: 0.0985\n",
            "🎉 New best backbone saved to vit_mae_adapted_backbone.pth with loss: 0.0985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MAE Epoch 3/25: 100%|██████████| 311/311 [01:05<00:00,  4.76it/s, Loss=0.1016]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Epoch 3 - Average Reconstruction Loss: 0.0985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MAE Epoch 4/25: 100%|██████████| 311/311 [01:05<00:00,  4.76it/s, Loss=0.1314]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Epoch 4 - Average Reconstruction Loss: 0.0987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MAE Epoch 5/25: 100%|██████████| 311/311 [01:05<00:00,  4.76it/s, Loss=0.1044]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Epoch 5 - Average Reconstruction Loss: 0.0986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MAE Epoch 6/25: 100%|██████████| 311/311 [01:05<00:00,  4.71it/s, Loss=0.1068]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Epoch 6 - Average Reconstruction Loss: 0.0986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MAE Epoch 7/25: 100%|██████████| 311/311 [01:06<00:00,  4.68it/s, Loss=0.1083]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Epoch 7 - Average Reconstruction Loss: 0.0984\n",
            "🎉 New best backbone saved to vit_mae_adapted_backbone.pth with loss: 0.0984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MAE Epoch 8/25: 100%|██████████| 311/311 [01:06<00:00,  4.69it/s, Loss=0.1020]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Epoch 8 - Average Reconstruction Loss: 0.0985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MAE Epoch 9/25: 100%|██████████| 311/311 [01:06<00:00,  4.69it/s, Loss=0.0820]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Epoch 9 - Average Reconstruction Loss: 0.0986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MAE Epoch 10/25: 100%|██████████| 311/311 [01:06<00:00,  4.69it/s, Loss=0.0754]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Epoch 10 - Average Reconstruction Loss: 0.0985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MAE Epoch 11/25: 100%|██████████| 311/311 [01:06<00:00,  4.70it/s, Loss=0.1259]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Epoch 11 - Average Reconstruction Loss: 0.0986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MAE Epoch 12/25: 100%|██████████| 311/311 [01:06<00:00,  4.68it/s, Loss=0.1017]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Epoch 12 - Average Reconstruction Loss: 0.0985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MAE Epoch 13/25: 100%|██████████| 311/311 [01:06<00:00,  4.70it/s, Loss=0.1031]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE Epoch 13 - Average Reconstruction Loss: 0.0986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MAE Epoch 14/25:  18%|█▊        | 56/311 [00:12<00:55,  4.62it/s, Loss=0.1137]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4151697103.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmae_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool_masked_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn_mae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0moptimizer_mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PART B: K-FOLD LINEAR PROBING ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🧠 PART B: STARTING K-FOLD SUPERVISED LINEAR PROBING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Define Classification Model and Dataset for Probing\n",
        "class ClassificationModel(nn.Module):\n",
        "    def __init__(self, backbone_path, num_classes):\n",
        "        super().__init__()\n",
        "        self.backbone = ViTModel.from_pretrained(MODEL_CHECKPOINT, add_pooling_layer=False)\n",
        "        # Load the state dictionary and remove unexpected keys\n",
        "        state_dict = torch.load(backbone_path)\n",
        "        # Remove keys related to the pooling layer if they exist\n",
        "        state_dict.pop('pooler.dense.weight', None)\n",
        "        state_dict.pop('pooler.dense.bias', None)\n",
        "        self.backbone.load_state_dict(state_dict)\n",
        "\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False # Freeze backbone\n",
        "        self.classifier = nn.Linear(self.backbone.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        outputs = self.backbone(pixel_values)\n",
        "        # Use the CLS token for classification\n",
        "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n",
        "        return logits\n",
        "\n",
        "class LabeledDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, processor):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.processor = processor\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['image'])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        pixel_values = self.processor(image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
        "        label = row['label']\n",
        "        return pixel_values, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# 2. Run K-Fold Loop\n",
        "labeled_df = full_df[full_df['none'].notna()].copy()\n",
        "labeled_df['label'] = np.argmax(labeled_df[CLASS_NAMES].values, axis=1)\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(labeled_df, labeled_df['label'])):\n",
        "    print(f\"\\n--- FOLD {fold + 1}/{N_SPLITS} ---\")\n",
        "    train_df = labeled_df.iloc[train_idx]\n",
        "    val_df = labeled_df.iloc[val_idx]\n",
        "\n",
        "    train_dataset = LabeledDataset(train_df, TRAIN_IMG_DIR, processor)\n",
        "    val_dataset = LabeledDataset(val_df, TRAIN_IMG_DIR, processor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=PROBE_BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=PROBE_BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = ClassificationModel(ADAPTED_BACKBONE_PATH, NUM_CLASSES).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.classifier.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_accuracy = 0\n",
        "    for epoch in range(PROBE_EPOCHS):\n",
        "        model.train()\n",
        "        for pixel_values, labels in tqdm(train_loader, desc=f\"Probing Epoch {epoch+1}\"):\n",
        "            pixel_values, labels = pixel_values.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(pixel_values)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for pixel_values, labels in val_loader:\n",
        "                pixel_values, labels = pixel_values.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = model(pixel_values)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), f\"vit_classifier_fold_{fold+1}.pth\")\n",
        "\n",
        "    print(f\"Fold {fold+1} Best Val Accuracy: {best_val_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ9KKFOb_G-R",
        "outputId": "9ca39d4b-b32e-4988-ec60-008906f6922d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🧠 PART B: STARTING K-FOLD SUPERVISED LINEAR PROBING\n",
            "============================================================\n",
            "\n",
            "--- FOLD 1/4 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Probing Epoch 1: 100%|██████████| 140/140 [00:25<00:00,  5.57it/s]\n",
            "Probing Epoch 2: 100%|██████████| 140/140 [00:24<00:00,  5.72it/s]\n",
            "Probing Epoch 3: 100%|██████████| 140/140 [00:24<00:00,  5.78it/s]\n",
            "Probing Epoch 4: 100%|██████████| 140/140 [00:24<00:00,  5.67it/s]\n",
            "Probing Epoch 5: 100%|██████████| 140/140 [00:24<00:00,  5.72it/s]\n",
            "Probing Epoch 6: 100%|██████████| 140/140 [00:24<00:00,  5.72it/s]\n",
            "Probing Epoch 7: 100%|██████████| 140/140 [00:24<00:00,  5.71it/s]\n",
            "Probing Epoch 8: 100%|██████████| 140/140 [00:24<00:00,  5.73it/s]\n",
            "Probing Epoch 9: 100%|██████████| 140/140 [00:24<00:00,  5.70it/s]\n",
            "Probing Epoch 10: 100%|██████████| 140/140 [00:24<00:00,  5.71it/s]\n",
            "Probing Epoch 11: 100%|██████████| 140/140 [00:24<00:00,  5.72it/s]\n",
            "Probing Epoch 12: 100%|██████████| 140/140 [00:24<00:00,  5.71it/s]\n",
            "Probing Epoch 13: 100%|██████████| 140/140 [00:24<00:00,  5.72it/s]\n",
            "Probing Epoch 14: 100%|██████████| 140/140 [00:24<00:00,  5.72it/s]\n",
            "Probing Epoch 15: 100%|██████████| 140/140 [00:24<00:00,  5.71it/s]\n",
            "Probing Epoch 16: 100%|██████████| 140/140 [00:24<00:00,  5.74it/s]\n",
            "Probing Epoch 17: 100%|██████████| 140/140 [00:24<00:00,  5.72it/s]\n",
            "Probing Epoch 18: 100%|██████████| 140/140 [00:23<00:00,  5.91it/s]\n",
            "Probing Epoch 19: 100%|██████████| 140/140 [00:24<00:00,  5.74it/s]\n",
            "Probing Epoch 20: 100%|██████████| 140/140 [00:24<00:00,  5.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Best Val Accuracy: 0.7582\n",
            "\n",
            "--- FOLD 2/4 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Probing Epoch 1: 100%|██████████| 140/140 [00:24<00:00,  5.78it/s]\n",
            "Probing Epoch 2: 100%|██████████| 140/140 [00:24<00:00,  5.80it/s]\n",
            "Probing Epoch 3: 100%|██████████| 140/140 [00:24<00:00,  5.81it/s]\n",
            "Probing Epoch 4: 100%|██████████| 140/140 [00:24<00:00,  5.80it/s]\n",
            "Probing Epoch 5: 100%|██████████| 140/140 [00:23<00:00,  5.91it/s]\n",
            "Probing Epoch 6: 100%|██████████| 140/140 [00:24<00:00,  5.75it/s]\n",
            "Probing Epoch 7: 100%|██████████| 140/140 [00:24<00:00,  5.79it/s]\n",
            "Probing Epoch 8: 100%|██████████| 140/140 [00:24<00:00,  5.77it/s]\n",
            "Probing Epoch 9: 100%|██████████| 140/140 [00:24<00:00,  5.78it/s]\n",
            "Probing Epoch 10: 100%|██████████| 140/140 [00:23<00:00,  5.95it/s]\n",
            "Probing Epoch 11: 100%|██████████| 140/140 [00:24<00:00,  5.80it/s]\n",
            "Probing Epoch 12: 100%|██████████| 140/140 [00:24<00:00,  5.79it/s]\n",
            "Probing Epoch 13: 100%|██████████| 140/140 [00:24<00:00,  5.78it/s]\n",
            "Probing Epoch 14: 100%|██████████| 140/140 [00:24<00:00,  5.81it/s]\n",
            "Probing Epoch 15: 100%|██████████| 140/140 [00:23<00:00,  5.94it/s]\n",
            "Probing Epoch 16: 100%|██████████| 140/140 [00:24<00:00,  5.69it/s]\n",
            "Probing Epoch 17: 100%|██████████| 140/140 [00:24<00:00,  5.72it/s]\n",
            "Probing Epoch 18: 100%|██████████| 140/140 [00:24<00:00,  5.71it/s]\n",
            "Probing Epoch 19: 100%|██████████| 140/140 [00:24<00:00,  5.73it/s]\n",
            "Probing Epoch 20: 100%|██████████| 140/140 [00:24<00:00,  5.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 Best Val Accuracy: 0.7408\n",
            "\n",
            "--- FOLD 3/4 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Probing Epoch 1: 100%|██████████| 140/140 [00:24<00:00,  5.72it/s]\n",
            "Probing Epoch 2: 100%|██████████| 140/140 [00:24<00:00,  5.70it/s]\n",
            "Probing Epoch 3: 100%|██████████| 140/140 [00:24<00:00,  5.72it/s]\n",
            "Probing Epoch 4: 100%|██████████| 140/140 [00:24<00:00,  5.71it/s]\n",
            "Probing Epoch 5: 100%|██████████| 140/140 [00:24<00:00,  5.71it/s]\n",
            "Probing Epoch 6: 100%|██████████| 140/140 [00:24<00:00,  5.73it/s]\n",
            "Probing Epoch 7: 100%|██████████| 140/140 [00:24<00:00,  5.74it/s]\n",
            "Probing Epoch 8: 100%|██████████| 140/140 [00:24<00:00,  5.70it/s]\n",
            "Probing Epoch 9: 100%|██████████| 140/140 [00:23<00:00,  5.91it/s]\n",
            "Probing Epoch 10: 100%|██████████| 140/140 [00:24<00:00,  5.74it/s]\n",
            "Probing Epoch 11: 100%|██████████| 140/140 [00:24<00:00,  5.75it/s]\n",
            "Probing Epoch 12: 100%|██████████| 140/140 [00:24<00:00,  5.77it/s]\n",
            "Probing Epoch 13: 100%|██████████| 140/140 [00:24<00:00,  5.79it/s]\n",
            "Probing Epoch 14: 100%|██████████| 140/140 [00:24<00:00,  5.78it/s]\n",
            "Probing Epoch 15: 100%|██████████| 140/140 [00:24<00:00,  5.78it/s]\n",
            "Probing Epoch 16: 100%|██████████| 140/140 [00:24<00:00,  5.73it/s]\n",
            "Probing Epoch 17: 100%|██████████| 140/140 [00:24<00:00,  5.74it/s]\n",
            "Probing Epoch 18: 100%|██████████| 140/140 [00:24<00:00,  5.78it/s]\n",
            "Probing Epoch 19: 100%|██████████| 140/140 [00:24<00:00,  5.75it/s]\n",
            "Probing Epoch 20: 100%|██████████| 140/140 [00:24<00:00,  5.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 Best Val Accuracy: 0.7495\n",
            "\n",
            "--- FOLD 4/4 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Probing Epoch 1: 100%|██████████| 140/140 [00:24<00:00,  5.77it/s]\n",
            "Probing Epoch 2: 100%|██████████| 140/140 [00:24<00:00,  5.69it/s]\n",
            "Probing Epoch 3: 100%|██████████| 140/140 [00:24<00:00,  5.70it/s]\n",
            "Probing Epoch 4: 100%|██████████| 140/140 [00:24<00:00,  5.71it/s]\n",
            "Probing Epoch 5: 100%|██████████| 140/140 [00:24<00:00,  5.68it/s]\n",
            "Probing Epoch 6: 100%|██████████| 140/140 [00:24<00:00,  5.71it/s]\n",
            "Probing Epoch 7: 100%|██████████| 140/140 [00:24<00:00,  5.73it/s]\n",
            "Probing Epoch 8: 100%|██████████| 140/140 [00:24<00:00,  5.73it/s]\n",
            "Probing Epoch 9: 100%|██████████| 140/140 [00:24<00:00,  5.71it/s]\n",
            "Probing Epoch 10: 100%|██████████| 140/140 [00:24<00:00,  5.71it/s]\n",
            "Probing Epoch 11: 100%|██████████| 140/140 [00:24<00:00,  5.71it/s]\n",
            "Probing Epoch 12: 100%|██████████| 140/140 [00:24<00:00,  5.72it/s]\n",
            "Probing Epoch 13: 100%|██████████| 140/140 [00:24<00:00,  5.72it/s]\n",
            "Probing Epoch 14: 100%|██████████| 140/140 [00:24<00:00,  5.73it/s]\n",
            "Probing Epoch 15: 100%|██████████| 140/140 [00:24<00:00,  5.74it/s]\n",
            "Probing Epoch 16: 100%|██████████| 140/140 [00:24<00:00,  5.75it/s]\n",
            "Probing Epoch 17: 100%|██████████| 140/140 [00:24<00:00,  5.75it/s]\n",
            "Probing Epoch 18: 100%|██████████| 140/140 [00:24<00:00,  5.75it/s]\n",
            "Probing Epoch 19: 100%|██████████| 140/140 [00:24<00:00,  5.72it/s]\n",
            "Probing Epoch 20: 100%|██████████| 140/140 [00:24<00:00,  5.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 Best Val Accuracy: 0.7480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- PART C: ENSEMBLE PREDICTION ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🏆 PART C: STARTING ENSEMBLE PREDICTION ON TEST SET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Prepare Test Data\n",
        "test_image_files = sorted([f for f in os.listdir(TEST_IMG_DIR) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, fnames, img_dir, processor):\n",
        "        self.fnames = fnames\n",
        "        self.img_dir = img_dir\n",
        "        self.processor = processor\n",
        "    def __len__(self): return len(self.fnames)\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.fnames[idx]\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        pixel_values = self.processor(image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
        "        return pixel_values, fname\n",
        "\n",
        "test_dataset = TestDataset(test_image_files, TEST_IMG_DIR, processor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=PROBE_BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 2. Generate and Average Predictions\n",
        "all_fold_probs = []\n",
        "for fold in range(N_SPLITS):\n",
        "    model = ClassificationModel(ADAPTED_BACKBONE_PATH, NUM_CLASSES).to(DEVICE)\n",
        "    model.load_state_dict(torch.load(f\"vit_classifier_fold_{fold+1}.pth\"))\n",
        "    model.eval()\n",
        "\n",
        "    fold_probs = []\n",
        "    with torch.no_grad():\n",
        "        for pixel_values, _ in tqdm(test_loader, desc=f\"Predicting with Fold {fold+1}\"):\n",
        "            pixel_values = pixel_values.to(DEVICE)\n",
        "            outputs = model(pixel_values)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            fold_probs.append(probs.cpu().numpy())\n",
        "    all_fold_probs.append(np.concatenate(fold_probs, axis=0))\n",
        "\n",
        "avg_probs = np.mean(all_fold_probs, axis=0)\n",
        "\n",
        "# 3. Save Submission File\n",
        "submission_df = pd.DataFrame()\n",
        "submission_df['image'] = test_image_files\n",
        "for i, class_name in enumerate(CLASS_NAMES):\n",
        "    submission_df[class_name] = avg_probs[:, i]\n",
        "\n",
        "submission_df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
        "print(f\"\\n✅ Submission file saved to {OUTPUT_CSV_PATH}\")\n",
        "print(\"\\nTop 5 rows of submission file:\")\n",
        "print(submission_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vdDZ4o4_KvK",
        "outputId": "1ec89798-409b-4bd0-ab0e-49ad70c26b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🏆 PART C: STARTING ENSEMBLE PREDICTION ON TEST SET\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Predicting with Fold 1: 100%|██████████| 180/180 [00:39<00:00,  4.58it/s]\n",
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Predicting with Fold 2: 100%|██████████| 180/180 [00:39<00:00,  4.55it/s]\n",
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Predicting with Fold 3: 100%|██████████| 180/180 [00:39<00:00,  4.50it/s]\n",
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Predicting with Fold 4: 100%|██████████| 180/180 [00:40<00:00,  4.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Submission file saved to submission_vit_mae_ensemble.csv\n",
            "\n",
            "Top 5 rows of submission file:\n",
            "        image      none  infection  ischaemia      both\n",
            "0  501000.jpg  0.117010   0.745947   0.008810  0.128234\n",
            "1  501001.jpg  0.007359   0.020343   0.638700  0.333597\n",
            "2  501002.jpg  0.335097   0.658787   0.002314  0.003802\n",
            "3  501003.jpg  0.383388   0.609733   0.002775  0.004104\n",
            "4  501004.jpg  0.058074   0.190404   0.597071  0.154451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PART C: ENSEMBLE PREDICTION ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🏆 PART C: STARTING ENSEMBLE PREDICTION ON TEST SET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Prepare Test Data\n",
        "test_image_files = sorted([f for f in os.listdir(TEST_IMG_DIR) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, fnames, img_dir, processor):\n",
        "        self.fnames = fnames\n",
        "        self.img_dir = img_dir\n",
        "        self.processor = processor\n",
        "    def __len__(self): return len(self.fnames)\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.fnames[idx]\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        pixel_values = self.processor(image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
        "        return pixel_values, fname\n",
        "\n",
        "test_dataset = TestDataset(test_image_files, TEST_IMG_DIR, processor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=PROBE_BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 2. Generate and Average Predictions\n",
        "all_fold_probs = []\n",
        "for fold in range(N_SPLITS):\n",
        "    model = ClassificationModel(ADAPTED_BACKBONE_PATH, NUM_CLASSES).to(DEVICE)\n",
        "    model.load_state_dict(torch.load(f\"vit_classifier_fold_{fold+1}.pth\"))\n",
        "    model.eval()\n",
        "\n",
        "    fold_probs = []\n",
        "    with torch.no_grad():\n",
        "        for pixel_values, _ in tqdm(test_loader, desc=f\"Predicting with Fold {fold+1}\"):\n",
        "            pixel_values = pixel_values.to(DEVICE)\n",
        "            outputs = model(pixel_values)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            fold_probs.append(probs.cpu().numpy())\n",
        "    all_fold_probs.append(np.concatenate(fold_probs, axis=0))\n",
        "\n",
        "avg_probs = np.mean(all_fold_probs, axis=0)\n",
        "\n",
        "# 3. Save Submission File\n",
        "submission_df = pd.DataFrame()\n",
        "submission_df['image'] = test_image_files\n",
        "for i, class_name in enumerate(CLASS_NAMES):\n",
        "    submission_df[class_name] = avg_probs[:, i]\n",
        "\n",
        "submission_df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
        "print(f\"\\n✅ Submission file saved to {OUTPUT_CSV_PATH}\")\n",
        "print(\"\\nTop 5 rows of submission file:\")\n",
        "print(submission_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaNMBsK0YGsC",
        "outputId": "c52e5eb2-b4b5-4ab5-ae8e-5689cdd3a4d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🏆 PART C: STARTING ENSEMBLE PREDICTION ON TEST SET\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Predicting with Fold 1: 100%|██████████| 180/180 [00:40<00:00,  4.48it/s]\n",
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Predicting with Fold 2: 100%|██████████| 180/180 [00:40<00:00,  4.45it/s]\n",
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Predicting with Fold 3: 100%|██████████| 180/180 [00:40<00:00,  4.48it/s]\n",
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Predicting with Fold 4: 100%|██████████| 180/180 [00:40<00:00,  4.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Submission file saved to submission_vit_mae_ensemble.csv\n",
            "\n",
            "Top 5 rows of submission file:\n",
            "        image      none  infection  ischaemia      both\n",
            "0  501000.jpg  0.117010   0.745947   0.008810  0.128234\n",
            "1  501001.jpg  0.007359   0.020343   0.638700  0.333597\n",
            "2  501002.jpg  0.335097   0.658787   0.002314  0.003802\n",
            "3  501003.jpg  0.383388   0.609733   0.002775  0.004104\n",
            "4  501004.jpg  0.058074   0.190404   0.597071  0.154451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}